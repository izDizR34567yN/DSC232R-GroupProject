# writeup scrap

## Required for README.md:
1. Intro (adapted from previous README.md submission by Andre)
   
   This project aims to analyze a Spotify Charts dataset and develop a machine learning model to create personalized playlists based on audio features of the tracks. The goal is to cluster songs into different categories and generate playlists that fit specific themes or moods. The dataset used in this project is the [Spotify Charts (All Audio Data)](https://www.kaggle.com/datasets/sunnykakar/spotify-charts-all-audio-data) dataset from Kaggle, which includes various attributes related to Spotify music tracks. Millions of observations with dozens of attributes render this dataset an unwieldy tangle of unrefined information, largely void of practical value in its raw form. By implementing scalable computational methods and technology, we aim to develop efficient model that can derive meaningful insights from an otherwise cryptic mass of information.

2. Complete submission of all previous submissions (**pending**...)
3. All code uploaded in the form of Jupyter notebooks that can be easily followed along to your GitHub repo (**pending**...)
4. A complete writeup that includes the following (**IP**...)
   1. Introduction of your project. Why chosen? why is it cool? General/Broader impact of having a good predictive mode. i.e. why is this important?

      see above I guess?

   2. Figures (of your choosing to help with the narration of your story) with legends (similar to a scientific paper) For reference you search machine learning and your model in google scholar for reference examples. (**pending**...)

      

   3. Methods section (this section will include the exploration results, preprocessing steps, models chosen in the order they were executed. Parameters chosen. Please make sub-sections for every step. i.e Data Exploration, Preprocessing, Model 1, Model 2, additional models are optional , (note models can be the same i.e. DNN but different versions of it if they are distinct enough. Changes can not be incremental). You can put links here to notebooks and/or code blocks using three ` in markup for displaying code. so it would look like this: ``` MY CODE BLOCK ```Note: A methods section does not include any why. the reason why will be in the discussion section. This is just a summary of your methods

      ### Data Exploration
  
         The dataset of interest accords to a simple DataFrame model and is available for [download](https://www.kaggle.com/datasets/sunnykakar/spotify-charts-all-audio-data) as a csv file on Kaggle. To account for its massive scale, we employ Apache Spark to facilitate efficient analysis of the dataset. We begin by rendering the dataset as a Spark DataFrame such that we may derive a preliminary overview of its contents. Thereafter, we are able to carry out a brief exploratory data analysis that includes inspecting and appropriately adjusting the dataset's schema, counting observations, generating summary statistics for numeric attributes, checking for missingness, and exploring select feature interactions through a series of simple scatterplots. The most significant results of this exploratory data analysis are outlined in the "Results" section of this report.
      
      ### Preprocessing

         Having completed a brief exploratory data analysis, we proceed to carry out appropriate preprocessing measures. This includes generic steps of restricting our interest to a relevant subset of select features, scaling numeric features, one-hot encoding categorical features, and dropping or imputing features with missing values.

      ### Model Selection

         Our chief purpose in analyzing this dataset is to develop a means for effectively categorizing its records without having been provided any such categorical labels in the dataset, the practical relevance of which may be realized, for example, through a playlist recommender. Clearly, therefore, our purpose fundamentally demands an unsupervised learning approach. With this in mind, we proceed to train and compare signature clustering models provided in the pyspark API, on the preprocessed dataset, such that any categories implicitly described by the dataset may be revealed. Namely, we compare the performances of KMeans, BisectingKMeans, and Gaussian Mixture models through an analysis of the silhouette scores each produces across different numbers of clusters, and we consider the results of this analysis to select an optimal model. Finally, we aim to exemplify the practical value of our analysis by building a playlist recommender upon one of our better-performing models.

      For further insights into the performance of each models, we randomly sample roughly 40 observations from each cluster generated by a given model. From each sample, we aim to infer the character of its respective cluster. Because manual inspection may be tedious or error-prone, we instead unify the samples (with cluster labels) into a single small-scale dataset on which we train a simple decision tree classifier--in scikit-learn--and plot the resulting decision tree to extract the per-cluster insights we seek. For simplicity, we set the maximum number of leaf nodes in this decision tree classifier equal to the number of clusters used. To estimate the accuracy of the categorization mechanism embodied a given clustering model, we calculate the proportion of correct predictions the decision tree makes on each cluster.

   5. Results section. This will include the results from the methods listed above (C). You will have figures here about your results as well. No exploration of results is done here. This is mainly just a summary of your results. The sub-sections will be the same as the sections in your methods section.

      ### Data Exploration

         Our exploratory data analysis revealed that our dataset comprises 26,174,269 observations in a 28-dimensional feature space. In raw form, the dataset enforces a simple, monotonic schema in which all features are nullable strings, including those features which are semantically numerical. To enable statistical summarization of such features, we modify the given schema, casting features which are evidently numerical from string to float data types.

         Our preliminary analysis further revealed that the dataset is mostly complete, with relatively few missing values. The percentage of missing values per attribute is well under 2%, and only one feature ("streams") contains a notable proportion (22.37%) of missing values. To avoid sampling bias on this feature--and because we expect the number of streams per record to be comparatively insignificant for our purposes--we simply exclude this feature from our future analyses.

         Finally, through select scatterplots, we seek a very generic level of insight concerning any potentially noteworthy relationships among features. While most pairs of features appear to be largely independent, a few pairs do exhibit an obvious degree of association that confirm sensible notions about them. For example, the following plot accurately reflects the physical definition of loudness as a logarithmic function of energy:

   ![dsc232r_finalprojim1](https://github.com/izDizR34567yN/DSC232R-GroupProject/assets/169011035/73faf648-9f5a-4bba-85d1-6fe1eecec41a)


      ### Model Selection

   Upon training a KMeans model on the dataset across nine different choices for number of clusters, we are able to generate the following elbow plot:

![dsc232r_finalprojim3](https://github.com/izDizR34567yN/DSC232R-GroupProject/assets/169011035/b6765169-39f5-4645-a9e1-eeaa084318d4)

   This plot clearly suggests that seven is the optimal number of clusters to choose when training a KMeans model on our dataset.

   In contrast, training a BisectingKMeans model on the dataset across the same range for number of clusters yields the following elbow plot:

![dsc232r_finalprojim4](https://github.com/izDizR34567yN/DSC232R-GroupProject/assets/169011035/416fed82-68a8-4ac0-b8c3-687662a8e17f)

   Unlike the KMeans model, the optimal number of clusters is somewhat less dramatically evident in this case. Still with reasonable judgment, one would very likely select eight as the optimal number of clusters for training a BisectingKMeans model on our dataset. By comparing this elbow plot with the one previously generated for KMeans models, one may further notice that KMeans models appear to be preferred over BisectingKMeans models for this dataset.

   In hopes of confirming these main findings, we again train KMeans models and BisectingKMeans models across the same range of number of clusters and calculate their silhouette scores. We also train Gaussian generative models over the same range. Because of the rather inconvenient computational expense associated with training Gaussian generative models, we have forgone the elbow method on Gaussian generative models, and we have deemed it worthwile to only consider Gaussian generative models in this silhouette score analysis. Ultimately, we were able to succefully generate the following compact plot of performances in terms of silhouette scores:

![dsc232r_finalprojim2](https://github.com/izDizR34567yN/DSC232R-GroupProject/assets/169011035/30a0ab05-a50c-4f58-9fc0-e204a70f5323)

   Quite fortunately, this plot exactly confirms our previous findings with respect to both KMeans and BisectingKMeans models. Namely, we have verified that seven is the preferred number of clusters for KMeans models; that eight is the preferred number of clusters for BisectingKMeans models; and that, overall, the performance of the KMeans model is generally somewhat better than that of the BisectingKMeans models for our dataset. Upon introducing Gaussian generative models in this analysis, we further find that they generally perform fairly poorly compared to the other two model types which we have evaluated on our dataset.


   6. Discussion section: This is where you will discuss the why, and your interpretation and your though process from beginning to end. This will mimic the sections you have created in your methods section as well as new sections you feel you need to create. You can also discuss how believable your results are at each step. You can discuss any short comings. It's ok to criticize as this shows your intellectual merit, as to how you are thinking about things scientifically and how you are able to correctly scrutinize things and find short comings. In science we never really find the perfect solution, especially since we know something will probably come up int he future (i.e. donkeys) and mess everything up. If you do it's probably a unicorn or the data and model you chose are just perfect for each other!

      ### Discussion

   7. Conclusion section: This is where you do a mind dump on your opinions and possible future directions. Basically what you wish you could have done differently. Here you close with final thoughts

      ### Conclusion

   8. Collaboration section: This is a statement of contribution by each member. This will be taken into consideration when making the final grade for each member in the group. Did you work as a team? was there a team leader? project manager? coding? writer? etc. Please be truthful about this as this will determine individual grades in participation. There is no job that is better than the other. If you did no code but did the entire write up and gave feedback during the steps and collaborated then you would still get full credit. If you only coded but gave feedback on the write up and other things, then you still get full credit. If you managed everyone and the deadlines and setup meetings and communicated with teaching staff only then you get full credit. Every role is important as long as you collaborated and were integral to the completion of the project. If the person did nothing. they risk getting a big fat 0. Just like in any job, if you did nothing, you have the risk of getting fired. Teamwork is one of the most important qualities in industry and academia!!! Start with Name: Title: Contribution. If the person contributed nothing then just put in writing: Did not participate in the project.

Your final model and final results summary will go in the last paragraph in of Part D.
