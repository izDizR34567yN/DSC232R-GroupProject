# writeup scrap

## Required for README.md:
1. Intro (adapted from previous README.md submission by Andre)
   
   This project aims to analyze a Spotify Charts dataset and develop a machine learning model to create personalized playlists based on audio features of the tracks. The goal is to cluster songs into different categories and generate playlists that fit specific themes or moods. The dataset used in this project is the [Spotify Charts (All Audio Data)](https://www.kaggle.com/datasets/sunnykakar/spotify-charts-all-audio-data) dataset from Kaggle, which includes various attributes related to Spotify music tracks. Millions of observations with dozens of attributes render this dataset an unwieldy tangle of unrefined information, largely void of practical value in its raw form. By implementing scalable computational methods and technology, we aim to develop efficient model that can derive meaningful insights from an otherwise cryptic mass of information.

2. Complete submission of all previous submissions (**pending**...)
3. All code uploaded in the form of Jupyter notebooks that can be easily followed along to your GitHub repo (**pending**...)
4. A complete writeup that includes the following (**IP**...)
   1. Introduction of your project. Why chosen? why is it cool? General/Broader impact of having a good predictive mode. i.e. why is this important?

      see above I guess?

   2. Figures (of your choosing to help with the narration of your story) with legends (similar to a scientific paper) For reference you search machine learning and your model in google scholar for reference examples. (**pending**...)

      

   3. Methods section (this section will include the exploration results, preprocessing steps, models chosen in the order they were executed. Parameters chosen. Please make sub-sections for every step. i.e Data Exploration, Preprocessing, Model 1, Model 2, additional models are optional , (note models can be the same i.e. DNN but different versions of it if they are distinct enough. Changes can not be incremental). You can put links here to notebooks and/or code blocks using three ` in markup for displaying code. so it would look like this: ``` MY CODE BLOCK ```Note: A methods section does not include any why. the reason why will be in the discussion section. This is just a summary of your methods

      ### Data Exploration
  
         The dataset of interest accords to a simple DataFrame model and is available for [download](https://www.kaggle.com/datasets/sunnykakar/spotify-charts-all-audio-data) as a csv file on Kaggle. To account for its massive scale, we employ Apache Spark to facilitate efficient analysis of the dataset. We begin by rendering the dataset as a Spark DataFrame such that we may derive a preliminary overview of its contents. Thereafter, we are able to carry out a brief exploratory data analysis that includes inspecting and appropriately adjusting the dataset's schema, counting observations, generating summary statistics for numeric attributes, checking for missingness, and exploring select feature interactions through a series of simple scatterplots. The most significant results of this exploratory data analysis are outlined in the "Results" section of this report.
      
      ### Preprocessing

         Having completed a brief exploratory data analysis, we proceed to carry out appropriate preprocessing measures. This includes generic steps of restricting our interest to a relevant subset of select features, scaling numeric features, one-hot encoding categorical features, and dropping or imputing features with missing values.

      ### Model 1

         Our chief purpose in analyzing this dataset is to develop a means for effectively categorizing its records without having been provided any such categorical labels in the dataset. I.e., our purpose fundamentally demands an unsupervised learning approach. With this in mind, we proceed to train a simple KMeans clustering model on the preprocessed dataset, such that any categories implicitly described by the dataset may be revealed. To decide on an optimal number of clusters, we compare the results yielded by the familiar elbow method and those yielded by an analysis of silhouette scores.

      Once a model is selected, we randomly sample from each cluster generated by the model. From each sample, we aim to infer the character of its respective cluster. Because manual inspection may be tedious or error-prone, we instead unify the samples (with cluster labels) into a single small-scale dataset on which we run a simple decision tree classifier (setting its maximum number of leaf nodes equal to the number of clusters used) to extract the per-cluster insights we seek. To estimate the accuracy of the categorization mechanism embodied by our clustering model, we run a hypothesis test on the proportion of correct predictions the decision tree makes on each cluster.

      ### Model 2
      
         (**pending...**)

   5. Results section. This will include the results from the methods listed above (C). You will have figures here about your results as well. No exploration of results is done here. This is mainly just a summary of your results. The sub-sections will be the same as the sections in your methods section.

      ### Data Exploration

      ### Preprocessing

      ### Model 1

      ### Model 2

   6. Discussion section: This is where you will discuss the why, and your interpretation and your though process from beginning to end. This will mimic the sections you have created in your methods section as well as new sections you feel you need to create. You can also discuss how believable your results are at each step. You can discuss any short comings. It's ok to criticize as this shows your intellectual merit, as to how you are thinking about things scientifically and how you are able to correctly scrutinize things and find short comings. In science we never really find the perfect solution, especially since we know something will probably come up int he future (i.e. donkeys) and mess everything up. If you do it's probably a unicorn or the data and model you chose are just perfect for each other!

      ### Discussion

   7. Conclusion section: This is where you do a mind dump on your opinions and possible future directions. Basically what you wish you could have done differently. Here you close with final thoughts

      ### Conclusion

   8. Collaboration section: This is a statement of contribution by each member. This will be taken into consideration when making the final grade for each member in the group. Did you work as a team? was there a team leader? project manager? coding? writer? etc. Please be truthful about this as this will determine individual grades in participation. There is no job that is better than the other. If you did no code but did the entire write up and gave feedback during the steps and collaborated then you would still get full credit. If you only coded but gave feedback on the write up and other things, then you still get full credit. If you managed everyone and the deadlines and setup meetings and communicated with teaching staff only then you get full credit. Every role is important as long as you collaborated and were integral to the completion of the project. If the person did nothing. they risk getting a big fat 0. Just like in any job, if you did nothing, you have the risk of getting fired. Teamwork is one of the most important qualities in industry and academia!!! Start with Name: Title: Contribution. If the person contributed nothing then just put in writing: Did not participate in the project.

Your final model and final results summary will go in the last paragraph in of Part D.
